{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was taken from http://ai.stanford.edu/~amaas/data/sentiment/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def files_to_dataframe(file_list):\n",
    "    lines = []\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                lines.append(line.strip())\n",
    "    \n",
    "    return pd.DataFrame(lines, columns=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_review(review, remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review, 'html.parser').get_text()\n",
    "    review_text = re.sub(r'[^a-zA-Z]', ' ', review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizing the data into the format used in http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_prefix = '/home/gclenden/Documents/umap_testing/data/aclImdb/'\n",
    "\n",
    "# Get labeled training files\n",
    "train_pos_files = glob.glob(os.path.join(data_prefix, 'train/pos/*.txt'))\n",
    "train_neg_files = glob.glob(os.path.join(data_prefix, 'train/neg/*.txt'))\n",
    "# Get labeled test files\n",
    "test_pos_files = glob.glob(os.path.join(data_prefix, 'test/pos/*.txt'))\n",
    "test_neg_files = glob.glob(os.path.join(data_prefix, 'test/neg/*.txt'))\n",
    "\n",
    "# Get unlabeled dataset\n",
    "unsup_files = glob.glob(os.path.join(data_prefix, 'train/unsup/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos = files_to_dataframe(train_pos_files)\n",
    "train_neg = files_to_dataframe(train_neg_files)\n",
    "\n",
    "test_pos = files_to_dataframe(test_pos_files)\n",
    "test_neg = files_to_dataframe(test_neg_files)\n",
    "\n",
    "unlabeled = files_to_dataframe(unsup_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos['review'] = train_pos['review'].apply(clean_review)\n",
    "train_neg['review'] = train_neg['review'].apply(clean_review)\n",
    "\n",
    "test_pos['review'] = test_pos['review'].apply(clean_review)\n",
    "test_neg['review'] = test_neg['review'].apply(clean_review)\n",
    "\n",
    "unlabeled['review'] = unlabeled['review'].apply(clean_review)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos['review'].to_csv('train-pos.txt', index=False)\n",
    "train_neg['review'].to_csv('train-neg.txt', index=False)\n",
    "test_pos['review'].to_csv('test-pos.txt', index=False)\n",
    "test_neg['review'].to_csv('test-neg.txt', index=False)\n",
    "unlabeled['review'].to_csv('train-unsup.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(line.decode().split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(line.decode().split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 348 ms, total: 14.2 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's actually train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 21s, sys: 20.6 s, total: 45min 42s\n",
      "Wall time: 12min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234701200"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.train(sentences.sentences_perm(), epochs=20, total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
